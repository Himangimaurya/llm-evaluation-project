<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM Evaluation Project | Himangi Maurya</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f5f5f5;
            color: #222;
        }
        header {
            background: #2c3e50;
            color: white;
            padding: 20px;
            text-align: center;
        }
        .container {
            max-width: 900px;
            margin: auto;
            padding: 20px;
            background: white;
            margin-top: 30px;
            border-radius: 8px;
            box-shadow: 0px 2px 10px rgba(0,0,0,0.1);
        }
        h2 {
            color: #2c3e50;
        }
        .code-box {
            background: #eee;
            padding: 15px;
            border-radius: 5px;
            font-family: monospace;
        }
        footer {
            text-align: center;
            margin-top: 40px;
            padding: 15px;
            background: #2c3e50;
            color: white;
        }
        a {
            color: #2980b9;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
    </style>
</head>

<body>

<header>
    <h1>LLM Evaluation Project</h1>
    <p>By Himangi Maurya</p>
</header>

<div class="container">

    <h2>ğŸ“Œ Overview</h2>
    <p>
        This project is a simple evaluation pipeline created to assess how well a Large Language Model (LLM) responds
        to a given user query. It analyzes the modelâ€™s answer for relevance, completeness, hallucination, and latency.
        This project was built as part of an internship assignment.
    </p>

    <h2>âš™ï¸ Features</h2>
    <ul>
        <li>Relevance scoring using embeddings and cosine similarity</li>
        <li>Completeness check based on keyword coverage</li>
        <li>Hallucination detection</li>
        <li>Latency calculation using timestamps</li>
        <li>Detailed JSON reports</li>
    </ul>

    <h2>ğŸ“ Project Structure</h2>
    <pre class="code-box">
EVALUATION/
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ evaluator.py
â”œâ”€â”€ utils.py
â”œâ”€â”€ sample_inputs/
â”œâ”€â”€ reports/
â”œâ”€â”€ README.md
â””â”€â”€ final_summary.md
    </pre>

    <h2>â–¶ï¸ How to Run</h2>
    <pre class="code-box">
python main.py --chat sample_inputs/chat.json --context sample_inputs/context.json --out report.json
    </pre>

    <h2>ğŸ“Š Example Output</h2>
    <p>A JSON report is generated with scores like:</p>
    <pre class="code-box">
{
  "relevance": 0.87,
  "completeness": 0.92,
  "hallucination": 0.10,
  "latency": 4.25
}
    </pre>

    <h2>ğŸ”— GitHub Repository</h2>
    <p>
        <a href="https://github.com/Himangimaurya/llm-evaluation-project" target="_blank">
            View the full project on GitHub
        </a>
    </p>

</div>

<footer>
    Â© 2025 Himangi Maurya â€” LLM Evaluation Project
</footer>

</body>
</html>
